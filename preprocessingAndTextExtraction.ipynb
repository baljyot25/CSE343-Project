{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\baljyot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baljyot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\baljyot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\baljyot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\baljyot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\baljyot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import contractions\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Read the JSON Lines file\n",
    "with open('../News_Category_Dataset_v3.json', 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "# Add commas between JSON objects and format as a list\n",
    "with open('output.json', 'w') as outfile:\n",
    "    outfile.write('[\\n')  # Start of JSON array\n",
    "    for i, line in enumerate(lines):\n",
    "        # Strip newline characters and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # Add comma at the end of each line except the last one\n",
    "        if i < len(lines) - 1:\n",
    "            outfile.write(line + ',\\n')\n",
    "        else:\n",
    "            outfile.write(line + '\\n')\n",
    "    outfile.write(']\\n')  # End of JSON array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=\"debjit_mod.csv\"\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in 'article_text': 3073\n"
     ]
    }
   ],
   "source": [
    "# Count the number of NaN values in the 'article_text' column\n",
    "nan_count = df['article_text'].isna().sum()\n",
    "print(f\"Number of NaNs in 'article_text': {nan_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>Index_Column</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/marni-for...</td>\n",
       "      <td>Marni Mayhem: The Indignity of Shopping Fast F...</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>I am often mortified (post acquisition) at wha...</td>\n",
       "      <td>Amy Tara Koch, Contributor\\nAuthor, travel wri...</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Author, travel writer and mom, lover of la dol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/star-wars...</td>\n",
       "      <td>These Incredible 3D Models Of Star Wars Land A...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Disney has unveiled a detailed look at its new...</td>\n",
       "      <td>Matthew Jacobs</td>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>1</td>\n",
       "      <td>Senior Entertainment Reporter, HuffPost Here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/family-re...</td>\n",
       "      <td>5 Keys to Cutting Your Mental and Emotional Um...</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>It is essential to recognize that these family...</td>\n",
       "      <td>Judith Johnson, Contributor\\nI am an author, m...</td>\n",
       "      <td>2012-05-24</td>\n",
       "      <td>2</td>\n",
       "      <td>I am an author, mentor and speaker who helps o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/how-to-ma...</td>\n",
       "      <td>Cooking For Kitchenphobes: How To Make Vinaigr...</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>Throw out those bottled dressings. They're no ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-02-28</td>\n",
       "      <td>3</td>\n",
       "      <td>Cooking For Kitchenphobes is a series aimed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/five-thin...</td>\n",
       "      <td>5 Things You Should Do Every Morning</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>You can try these simple steps to keep your mo...</td>\n",
       "      <td>Andrea Metcalf, Contributor\\nHealthy Living, C...</td>\n",
       "      <td>2012-02-08</td>\n",
       "      <td>4</td>\n",
       "      <td>Healthy Living, Certified Trainer, Author, Spe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/marni-for...   \n",
       "1  https://www.huffingtonpost.com/entry/star-wars...   \n",
       "2  https://www.huffingtonpost.com/entry/family-re...   \n",
       "3  https://www.huffingtonpost.com/entry/how-to-ma...   \n",
       "4  https://www.huffingtonpost.com/entry/five-thin...   \n",
       "\n",
       "                                            headline        category  \\\n",
       "0  Marni Mayhem: The Indignity of Shopping Fast F...  STYLE & BEAUTY   \n",
       "1  These Incredible 3D Models Of Star Wars Land A...   ENTERTAINMENT   \n",
       "2  5 Keys to Cutting Your Mental and Emotional Um...        WELLNESS   \n",
       "3  Cooking For Kitchenphobes: How To Make Vinaigr...    FOOD & DRINK   \n",
       "4               5 Things You Should Do Every Morning        WELLNESS   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  I am often mortified (post acquisition) at wha...   \n",
       "1  Disney has unveiled a detailed look at its new...   \n",
       "2  It is essential to recognize that these family...   \n",
       "3  Throw out those bottled dressings. They're no ...   \n",
       "4  You can try these simple steps to keep your mo...   \n",
       "\n",
       "                                             authors        date  \\\n",
       "0  Amy Tara Koch, Contributor\\nAuthor, travel wri...  2012-03-12   \n",
       "1                                     Matthew Jacobs  2017-07-14   \n",
       "2  Judith Johnson, Contributor\\nI am an author, m...  2012-05-24   \n",
       "3                                                NaN  2013-02-28   \n",
       "4  Andrea Metcalf, Contributor\\nHealthy Living, C...  2012-02-08   \n",
       "\n",
       "   Index_Column                                       article_text  \n",
       "0             0  Author, travel writer and mom, lover of la dol...  \n",
       "1             1  Senior Entertainment Reporter, HuffPost Here i...  \n",
       "2             2  I am an author, mentor and speaker who helps o...  \n",
       "3             3  Cooking For Kitchenphobes is a series aimed to...  \n",
       "4             4  Healthy Living, Certified Trainer, Author, Spe...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi name name number break\n",
      "example\n"
     ]
    }
   ],
   "source": [
    "# Function to remove the punctuation marks from the given text\n",
    "def remove_punctuation(text):\n",
    "\n",
    "    # Returning the cleaned text for further processing\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Function to remove the stopwords from the given text\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    # Defining a set of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenizing the given text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing the stop words from the given text based on the above created English stopword set\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Combining tokens back into a single string\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    # Returning the cleaned text for further processing\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to lemmatize the given text\n",
    "def lemmatize_text(text):\n",
    "\n",
    "    # Initializing the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenizing the given text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatizing each token of the given text to break them down to their root forms\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Combining the lemmatized tokens back into a single string abd returning it for further processing\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Function to remove URLs from the given text\n",
    "def remove_urls(text):\n",
    "\n",
    "    # Defining the URL pattern to remove from the given text\n",
    "    url_pattern = r'(?:(?:http|https|ftp|ftps|sftp|file|mailto|tel|ws|wss)://|www\\.)[a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+)+[^\\s]*'\n",
    "\n",
    "    # Returning the new text with all URLs removed for further processing\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "# Function to remove the html tags (if any) from the given text\n",
    "def remove_html_tags(text):\n",
    "\n",
    "    # Creating a BeautifulSoup object and parsing the text with html.parser\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Getting the text without HTML tags and returning it for further processing\n",
    "    return soup.get_text()\n",
    "\n",
    "# Function to expand contractions (for example: \"don't\" to \"do not\")\n",
    "def expand_contractions(text):\n",
    "\n",
    "    # Returning the new text with expanded words for further processing\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Function to remove the numbers from the given text\n",
    "def remove_numbers(text):\n",
    "    \n",
    "    # Returning the new text with numbers removed for further processing\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Function to keep only valid English words in the given text\n",
    "def remove_invalid_words(text):\n",
    "\n",
    "    # Tokenizing the given text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # # Get the list of valid English words\n",
    "    word_list = set(words.words())\n",
    "\n",
    "    # Checking validity for each word\n",
    "    valid_words = [word for word in tokens if word in word_list]\n",
    "\n",
    "    # Returning the list of valid English words extracted from the given text for further processing\n",
    "    return ' '.join(valid_words)\n",
    "\n",
    "#  Preprocessing function for cleaning the article's text for further processing\n",
    "def preprocess_text(text, to_remove_stopwords = True, lemmatize = True):\n",
    "\n",
    "    # Converting all the article's text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing all the urls from the article's text (keeping this before removing punctuation to avoid problems)\n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # Removing all the punctuation marks from the article's text\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    text = remove_html_tags(text) # Removing the html tags (if any)\n",
    "    text = expand_contractions(text) # Expanding contractions (For example: \"don't\" to \"do not\")\n",
    "    text = remove_numbers(text) # Removing numbers from the text\n",
    "\n",
    "    text = remove_invalid_words(text)\n",
    "\n",
    "    # Removing stop words (optional: you can keep this to improve the model's performance)\n",
    "    if(to_remove_stopwords):\n",
    "        text = remove_stopwords(text)\n",
    "    \n",
    "    # Lemmatizing the words to break them to their root meanings\n",
    "    if(lemmatize):\n",
    "        text = lemmatize_text(text)    \n",
    "\n",
    "    # Returning the final clean text\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test Example for the article text preprocessing function\n",
    "text = \"Hi! My name is abc. What is your name? my number is 123, https://www.youtube.com/ dlfjk kdkd dkdkdkd  <break> don't hadn't won't  \"\n",
    "text2 = \"using https://www.google.com/ as an example\"\n",
    "\n",
    "ptext = preprocess_text(text)\n",
    "ptext2 = preprocess_text(text2)\n",
    "print(ptext)\n",
    "print(ptext2)\n",
    "\n",
    "# Main implementation for the rows of the article dataframe\n",
    "# df['cleaned_text'] = df['article_text'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Fill NaN values in 'article_text'\n",
    "# df['article_text'] = df['article_text'].fillna('')\n",
    "df['final'] = df['final'].fillna('')\n",
    "\n",
    "# Apply the preprocess_text function with a progress bar\n",
    "df['cleaned_final'] = df['cleaned_article_text'].progress_apply(lambda x: preprocess_text(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the DataFrame with the new 'cleaned_article_text' column\n",
    "# print(df[['article_text', 'cleaned_article_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Component 0  Component 1  Component 2  Component 3  Component 4  \\\n",
      "0        0.198563     0.036805     0.015591     0.176804     0.000000   \n",
      "1        0.000916     0.554903     0.000000     0.202001     0.000000   \n",
      "2        0.058780     0.055621     0.000000     0.179734     0.000000   \n",
      "3        0.000000     0.000000     0.000000     0.200802     0.010664   \n",
      "4        0.000000     0.035482     0.004741     0.111430     0.000000   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "9995     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "9996     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "9997     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "9998     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "9999     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "\n",
      "      Component 5  Component 6  Component 7  Component 8  Component 9  \\\n",
      "0        0.000000     0.046612     0.000000     0.075800     0.215298   \n",
      "1        0.015642     0.055912     0.000000     0.009664     0.001326   \n",
      "2        0.070752     0.000000     0.000000     0.112520     0.494854   \n",
      "3        0.000000     0.098952     0.000000     0.000000     0.064438   \n",
      "4        0.046340     0.000000     0.015255     0.000000     0.318063   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "9995     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "9996     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "9997     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "9998     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "9999     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "\n",
      "      Component 10  Component 11  Component 12  Component 13  Component 14  \n",
      "0         0.124375       0.13955      0.111321      0.000000      0.008458  \n",
      "1         0.029850       0.00000      0.013175      0.000000      0.037203  \n",
      "2         0.000000       0.00000      0.061995      0.030679      0.022426  \n",
      "3         0.421769       0.00000      0.000000      0.000000      0.000000  \n",
      "4         0.283196       0.06383      0.000000      0.084717      0.079102  \n",
      "...            ...           ...           ...           ...           ...  \n",
      "9995      0.000000       0.00000      0.000000      0.000000      0.000000  \n",
      "9996      0.000000       0.00000      0.000000      0.000000      0.000000  \n",
      "9997      0.000000       0.00000      0.000000      0.000000      0.000000  \n",
      "9998      0.000000       0.00000      0.000000      0.000000      0.000000  \n",
      "9999      0.000000       0.00000      0.000000      0.000000      0.000000  \n",
      "\n",
      "[10000 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the number of topics for NMF\n",
    "n_topics = 15\n",
    "\n",
    "# Initialize NMF model\n",
    "nmf_model = NMF(n_components=15, random_state=42)\n",
    "\n",
    "# Fit the NMF model to the combined matrix\n",
    "nmf_features = nmf_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "# Create a DataFrame for the NMF features\n",
    "nmf_features_df = pd.DataFrame(nmf_features, columns=[f'Component {i}' for i in range(n_topics)])\n",
    "\n",
    "# Adding the original article IDs to the NMF features dataframe\n",
    "# nmf_features_df['id'] = bm25_features_df['id']\n",
    "print(nmf_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like play football park .'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"I like to play football in the park.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
